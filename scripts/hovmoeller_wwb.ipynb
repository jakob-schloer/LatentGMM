{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Hovmoeller diagram of high and low frequency winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as ctp\n",
    "import locale\n",
    "\n",
    "from latgmm.utils import utenso, preproc, eof, utdata, utstats, metric\n",
    "import latgmm.geoplot as gpl\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, \"en_US.utf8\")\n",
    "plt.style.use(\"../paper.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u10_hf = xr.open_dataset(\"../data/reanalysis/6-hourly/ERA5/era5_u10_anom_HF_1940_2021_5N5S_130E80W_1deg.nc\")['u10_anom_HF']\n",
    "u10_hf = u10_hf.assign_coords(lon=(((u10_hf.lon + 180) % 360) - 180))\n",
    "u10_hf = preproc.set_antimeridian2zero(u10_hf)\n",
    "u10_lf = xr.open_dataset(\"../data/reanalysis/6-hourly/ERA5/era5_u10_anom_LF_1940_2021_5N5S_130E80W_1deg.nc\")['u10_anom_LF']\n",
    "u10_lf = u10_lf.assign_coords(lon=(((u10_lf.lon + 180) % 360) - 180))\n",
    "u10_lf = preproc.set_antimeridian2zero(u10_lf)\n",
    "\n",
    "u10_ds = xr.merge([u10_hf, u10_lf])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SST data\n",
    "# =======================================================================================\n",
    "lon_range=[130, -70]\n",
    "lat_range=[-32, 32]\n",
    "\n",
    "sst_fname = \"../data/reanalysis/monthly/ERA5/sea_surface_temperature_era5_monthly_sp_1940-2022_1.0x1.0.nc\"\n",
    "\n",
    "# SST anomalies\n",
    "ssta = preproc.process_data(\n",
    "        sst_fname, vars=['sst'], antimeridian=True,\n",
    "        climatology='month', normalization=None, detrend_from=1950, \n",
    "        lon_range=lon_range, lat_range=lat_range,\n",
    ")['ssta']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single event Hovmoeller"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paperplot of EP and EEN events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "events = [\n",
    "    [1994, 1995],\n",
    "    [2006, 2007],\n",
    "]\n",
    "plotspec={\n",
    "    'u10_anom_HF': dict(cmap='RdYlBu_r', vmin=-3, vmax=3, step=.2, centercolor=\"#FFFFFF\"),\n",
    "    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=-3, vmax=3, step=.2, centercolor=\"#FFFFFF\"),\n",
    "}\n",
    "\n",
    "#events = [\n",
    "#    [1982, 1983],\n",
    "#    [1997, 1998],\n",
    "#    [2015, 2016],\n",
    "#]\n",
    "#plotspec={\n",
    "#    'u10_anom_HF': dict(cmap='RdYlBu_r', vmin=-5, vmax=5, step=.5, centercolor=\"#FFFFFF\"),\n",
    "#    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=-5, vmax=5, step=.5, centercolor=\"#FFFFFF\"),\n",
    "#}\n",
    "\n",
    "lat_range=[-5, 5]\n",
    "\n",
    "n_columns = len(u10_ds.data_vars) \n",
    "n_rows = len(events)\n",
    "height_ratios = [9]*(n_rows)\n",
    "height_ratios.append(1)\n",
    "fig = plt.figure(figsize=(n_columns*4.5, n_rows*2 + 0.2))\n",
    "gs = fig.add_gridspec(n_rows + 1, n_columns,\n",
    "                      height_ratios=height_ratios,\n",
    "                      hspace=0.3, wspace=0.2)\n",
    "\n",
    "central_longitude = 180\n",
    "proj = ctp.crs.PlateCarree(central_longitude=central_longitude)\n",
    "\n",
    "axs = []\n",
    "ims = []\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_columns):\n",
    "        var = list(u10_ds.data_vars)[j]\n",
    "        lat_mean = u10_ds[var].sel(\n",
    "            time=slice(f\"{events[i][0]}-01-01\", f\"{events[i][1]}-12-30\"),\n",
    "            lat=slice(lat_range[0], lat_range[1])\n",
    "        ).mean(dim='lat')\n",
    "\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        im = gpl.plot_hovmoeller(lat_mean, x='lon', y='time',\n",
    "                                 ax=ax, **plotspec[var], add_bar=False)\n",
    "\n",
    "        # X-axis\n",
    "        ax.set_xlim(None, 100)\n",
    "\n",
    "        if i < n_rows -1:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            xaxis_formatter = FuncFormatter(lambda x, pos: f'{np.where(x < 0, (x % 180),(x % 180 - 180)):.0f}')\n",
    "            ax.xaxis.set_major_formatter(xaxis_formatter)\n",
    "            ax.set_xlabel('lon')\n",
    "        \n",
    "        # Y-axis to month\n",
    "        ax.yaxis.set_major_formatter(mpl.dates.DateFormatter('%b'))\n",
    "\n",
    "        axs.append(ax)\n",
    "        ims.append(im)\n",
    "    \n",
    "# Add figure numbering\n",
    "#_ = gpl.enumerate_subplots(np.array(axs).T, pos_x=0.02, pos_y=1.05, fontsize=14)\n",
    "        \n",
    "\n",
    "# Shared colorbar\n",
    "cbar_labels = ['zonal wind anom. (HF)', 'zonal wind anom. (LF)']\n",
    "for j in range(n_columns):\n",
    "    ax = fig.add_subplot(gs[-1, j])\n",
    "    cbar = plt.colorbar(ims[j]['im'], orientation='horizontal', shrink=0.9,\n",
    "                        cax=ax, extend='both')\n",
    "    cbar.set_label(cbar_labels[j], size=14)\n",
    "\n",
    "    formatter = FuncFormatter(lambda x, pos: f'{x:.1f}')\n",
    "    cbar.ax.xaxis.set_major_formatter(formatter)\n",
    "    cbar.ax.yaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Hovmoeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "weight_events = xr.open_dataset(\n",
    "    \"../output/pcgmm/pcgmm_weights_mean_sst.nc\"\n",
    ")['__xarray_dataarray_variable__']\n",
    "gmm_order = weight_events.attrs['gmm_sort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def weighted_mean_over_years(ds: xr.Dataset, weights: xr.DataArray,\n",
    "                             mon_first_year: int=1, mon_second_year: int=12, \n",
    "                             ) -> xr.Dataset:\n",
    "    \"\"\"Compute a weighted mean over all events for a time-period around\n",
    "    the definition of the weight.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): _description_\n",
    "        weights (xr.DataArray): _description_\n",
    "        mon_first_year (int, optional): _description_. Defaults to 1.\n",
    "        mon_second_year (int, optional): _description_. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: _description_\n",
    "    \"\"\"\n",
    "    # Make sure dstaset for composites are on the same time points as weights\n",
    "    ds['time'] = np.array(ds['time'].data, dtype='datetime64[h]')\n",
    "    weights['time'] = np.array(weights['time'].data, dtype='datetime64[h]')\n",
    "\n",
    "    tmax = ds['time'].max() if ds['time'].max(\n",
    "    ) < weights['time'].max() else weights['time'].max()\n",
    "    tmin = ds['time'].min() if ds['time'].min(\n",
    "    ) > weights['time'].min() else weights['time'].min()\n",
    "    weights = weights.sortby(weights['time'])\n",
    "    weights = weights.sel(time=slice(tmin.data, tmax.data))\n",
    "    ds = ds.sel(time=slice(tmin.data, tmax.data))\n",
    "\n",
    "    # Create weighted mean for each ENSO events time period\n",
    "    weighted_mean = []\n",
    "    for var in list(ds.data_vars):\n",
    "        weighted_mean_var = []\n",
    "        da = ds[var]\n",
    "\n",
    "        years = np.unique(np.array(weights['time'].data, dtype='datetime64[Y]'))[:-2]\n",
    "        data_years = []\n",
    "        for y in years:\n",
    "            # Select time-period each year\n",
    "            tstart = np.datetime64(f\"{y}-{mon_first_year:02d}-01\", 'D')\n",
    "            tend = np.datetime64(f\"{y+1}-{mon_second_year:02d}-30\", 'D') \n",
    "            da_year = da.sel(time=slice(tstart, tend))\n",
    "            # Remove 29th of February if it's a leap year\n",
    "            da_year = da_year.where(~((da_year.time.dt.month == 2) & (da_year.time.dt.day == 29)), drop=True)\n",
    "\n",
    "            data_years.append(da_year.data)\n",
    "\n",
    "        # Create Dataarray\n",
    "        common_time = da_year['time'].data\n",
    "        data_years = xr.DataArray(np.array(data_years),\n",
    "                                  coords={'samples': np.arange(len(years)),\n",
    "                                          'time': common_time,\n",
    "                                          'lat': ds['lat'], 'lon': ds['lon']})\n",
    "        \n",
    "        # Loop over categories\n",
    "        for i, k in enumerate(weights['classes'].data):\n",
    "            weight_class = weights.sel(classes=k)\n",
    "\n",
    "            weight_years = [] \n",
    "            for y in years:\n",
    "                weight_years.append(weight_class.sel(time=f\"{y}-12-01\", method='nearest').data)\n",
    "\n",
    "            weight_years = xr.DataArray(np.array(weight_years),\n",
    "                                        coords={'samples': np.arange(len(years))})\n",
    "\n",
    "            # Weighted years\n",
    "            da_weighted = data_years.weighted(weight_years)\n",
    "            mean = da_weighted.mean(dim='samples')\n",
    "            weighted_mean_var.append(mean)\n",
    "\n",
    "        # Concat classes\n",
    "        weighted_mean_var = xr.concat(weighted_mean_var, dim=weights['classes'])\n",
    "        weighted_mean_var.name = var\n",
    "        weighted_mean.append(weighted_mean_var)\n",
    "\n",
    "\n",
    "    # Merge different vars\n",
    "    weighted_mean = xr.merge(weighted_mean)\n",
    "\n",
    "    return weighted_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute weighted mean over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_first_year = 1\n",
    "mon_second_year = 12\n",
    "lats = [-5, 5]\n",
    "weighted_mean = weighted_mean_over_years(\n",
    "    u10_ds, weights=weight_events, \n",
    "    mon_first_year=mon_first_year, mon_second_year=mon_second_year,\n",
    "    )\n",
    "weighted_lat_mean = weighted_mean.sel(lat=slice(lats[0], lats[1])).mean(dim='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plotspec={\n",
    "    'u10_anom_HF': dict(cmap='RdYlBu_r', vmin=None, vmax=None, step=.1, centercolor=\"#FFFFFF\"),\n",
    "    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=None, vmax=None, step=.1, centercolor=\"#FFFFFF\"),\n",
    "}\n",
    "n_columns = len(weighted_lat_mean.data_vars)\n",
    "n_rows = len(weighted_lat_mean['classes'])\n",
    "height_ratios = [9]*(n_rows)\n",
    "height_ratios.append(1)\n",
    "fig = plt.figure(figsize=(n_columns*6, n_rows*3 + 0.2))\n",
    "gs = fig.add_gridspec(n_rows + 1, n_columns,\n",
    "                      height_ratios=height_ratios,\n",
    "                      hspace=0.3, wspace=0.2)\n",
    "axs = []\n",
    "for j in range(n_columns):\n",
    "    for i in range(n_rows):\n",
    "        idx = gmm_order[i] + 1\n",
    "        var = list(weighted_lat_mean.data_vars)[j]\n",
    "        lat_mean = weighted_lat_mean[var].sel(classes=idx)\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        \n",
    "        im = gpl.plot_hovmoeller(lat_mean, x='lon', y='time',\n",
    "                                 ax=ax, **plotspec[var], add_bar=False,\n",
    "                                 cbkwargs=dict(orientation='vertical', shrink=0.8))\n",
    "\n",
    "        # X-axis\n",
    "        ax.set_xlim(None, 100)\n",
    "        xaxis_formatter = FuncFormatter(lambda x, pos: f'{np.where(x < 0, (x % 180),(x % 180 - 180)):.0f}')\n",
    "        ax.xaxis.set_major_formatter(xaxis_formatter)\n",
    "        \n",
    "        if i < n_rows -1:\n",
    "            ax.set_xticklabels([])\n",
    "        \n",
    "        # Y-axis to month\n",
    "        ax.yaxis.set_major_formatter(mpl.dates.DateFormatter('%b'))\n",
    "        \n",
    "\n",
    "    # Shared colorbar\n",
    "    ax = fig.add_subplot(gs[-1, j])\n",
    "    cbar = plt.colorbar(im['im'], orientation='horizontal', shrink=0.3,\n",
    "                        cax=ax, extend='both')\n",
    "    cbar.set_label(f\"{var}\", size=16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data and compute means of neutral years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sst = sst_fname\n",
    "null_hypothesis = 'neutral'\n",
    "n_samples_mean = 100\n",
    "n_samples_time = 12\n",
    "\n",
    "# Null hypothesis\n",
    "enso_classes = utenso.get_enso_flavors_obs(\n",
    "    definition='N3N4', fname=f_sst, vname='sst', climatology='month',\n",
    "    month_range=[12, 2],\n",
    ")\n",
    "if null_hypothesis == 'neutral':\n",
    "    # Neutral years as null hypothesis\n",
    "    time_snippets_null = np.array(\n",
    "        [enso_classes.loc[enso_classes['type'] == 'Normal']['start'],\n",
    "         enso_classes.loc[enso_classes['type'] == 'Normal']['end']]\n",
    "    ).T\n",
    "elif null_hypothesis == 'all':\n",
    "    # All winters as null hypothesis\n",
    "    time_snippets_null = np.array(\n",
    "        [enso_classes['start'],\n",
    "         enso_classes['end']]\n",
    "    ).T\n",
    "else:\n",
    "    raise ValueError(f\"Unknown null hypothesis: {null_hypothesis}\")\n",
    "\n",
    "samples_null = []\n",
    "for var in u10_ds.data_vars:\n",
    "    da = u10_ds[var]\n",
    "\n",
    "    # Select years and rearrange\n",
    "    years = np.unique(np.array(time_snippets_null[:,0], dtype='datetime64[Y]'))[:-1]\n",
    "    print(f\"Number of years: {len(years)}\")\n",
    "    da_null = []\n",
    "    for y in years:\n",
    "        # Select time-period each year\n",
    "        tstart = np.datetime64(f\"{y}-{mon_first_year:02d}-01\", 'D')\n",
    "        tend = np.datetime64(f\"{y+1}-{mon_second_year:02d}-30\", 'D') \n",
    "        da_year = da.sel(time=slice(tstart, tend))\n",
    "        # Remove 29th of February if it's a leap year\n",
    "        da_year = da_year.where(~((da_year.time.dt.month == 2) & (da_year.time.dt.day == 29)), drop=True)\n",
    "\n",
    "        da_null.append(da_year.data)\n",
    "\n",
    "    # Create Dataarray\n",
    "    common_time = da_year['time'].data\n",
    "    da_null = xr.DataArray(np.array(da_null),\n",
    "                              coords={'years': np.arange(len(years)),\n",
    "                                      'time': common_time,\n",
    "                                      'lat': u10_ds['lat'], 'lon': u10_ds['lon']})\n",
    "\n",
    "    # Sample means from null hypothesis\n",
    "    samples_null_var = []\n",
    "    for n in range(n_samples_mean):\n",
    "        samples_idx = np.random.choice(da_null['years'], size=n_samples_time,\n",
    "                                        replace=True)\n",
    "        samples_null_var.append(da_null.sel(\n",
    "            years=samples_idx).mean(dim='years'))\n",
    "\n",
    "    samples_null_var = xr.concat(samples_null_var, dim='samples')\n",
    "    samples_null_var.name = var\n",
    "    samples_null.append(samples_null_var)\n",
    "\n",
    "samples_null = xr.merge(samples_null)\n",
    "# Compute lat mean for hovmoeller\n",
    "samples_null_lat_mean = samples_null.sel(lat=slice(lats[0], lats[1])).mean(dim='lat')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "multiple_testing='dunn'\n",
    "\n",
    "masks = []\n",
    "pvalues = []\n",
    "for var in list(weighted_lat_mean.data_vars):\n",
    "    print(f\"Percentile of score for {var}\")\n",
    "    # Compute poc in parallel for classes \n",
    "    n_processes = len(weighted_lat_mean['classes'])\n",
    "    results = Parallel(n_jobs=5)(\n",
    "        delayed(utstats.percentile_of_scores)(\n",
    "            samples_null_lat_mean[var], weighted_lat_mean[var].isel(classes=i), \n",
    "            stackdim=['time', 'lon'], id=i)\n",
    "        for i in tqdm(range(n_processes))\n",
    "    )\n",
    "    # Read results\n",
    "    indices = []\n",
    "    pvals = []\n",
    "    for r in results:\n",
    "        p, i = r\n",
    "        indices.append(i)\n",
    "        pvals.append(p)\n",
    "    pvals = xr.concat(pvals, dim=weighted_lat_mean['classes'][indices])\n",
    "    pvalues.append(pvals)\n",
    "\n",
    "    mask_var = []\n",
    "    for k in pvals['classes']:\n",
    "        mask_var.append(utstats.field_significance_mask(\n",
    "            pvals.sel(classes=k), stackdim=('time', 'lon'), alpha=alpha, corr_type=multiple_testing)\n",
    "        )\n",
    "    # Concatenate along classes-dim\n",
    "    mask_var = xr.concat(mask_var, dim=pvals['classes'])\n",
    "    mask_var.name = var\n",
    "    masks.append(mask_var)\n",
    "\n",
    "pvalues = xr.merge(pvalues)\n",
    "mask = xr.merge(masks)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot weighted hovmoeller averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plotspec={\n",
    "    'u10_anom_HF': dict(cmap='RdYlBu_r', vmin=-1.5, vmax=1.5, step=.1, centercolor=\"#FFFFFF\"),\n",
    "    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=-1.5, vmax=1.5, step=.1, centercolor=\"#FFFFFF\"),\n",
    "}\n",
    "\n",
    "n_rows = len(weighted_lat_mean.data_vars)\n",
    "n_columns = len(weighted_lat_mean['classes']) - 2\n",
    "width_ratios = [19]*(n_columns)\n",
    "width_ratios.append(1)\n",
    "fig = plt.figure(figsize=(12, n_rows*3))\n",
    "gs = fig.add_gridspec(n_rows, n_columns+1, width_ratios=width_ratios,\n",
    "                      hspace=0.1, wspace=0.15)\n",
    "axs = []\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_columns):\n",
    "        idx = gmm_order[j+2] + 1\n",
    "        var = list(weighted_lat_mean.data_vars)[i]\n",
    "        lat_mean = weighted_lat_mean[var].sel(classes=idx)\n",
    "        # Statistical significance\n",
    "        lat_mean_masked = xr.where(mask[var].sel(classes=idx) == True, lat_mean, np.nan)\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        \n",
    "        im = gpl.plot_hovmoeller(lat_mean_masked, x='lon', y='time',\n",
    "                                 ax=ax, **plotspec[var], add_bar=False)\n",
    "\n",
    "        # X-axis\n",
    "        ax.set_xlim(None, 100)\n",
    "        if i < n_rows -1:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            xaxis_formatter = FuncFormatter(lambda x, pos: f'{np.where(x < 0, (x % 180),(x % 180 - 180)):.0f}')\n",
    "            ax.xaxis.set_major_formatter(xaxis_formatter)\n",
    "            ax.set_xlabel('longitude') \n",
    "        \n",
    "        # Y-axis to month\n",
    "        if j > 0:\n",
    "            ax.set_yticklabels([])\n",
    "        else:\n",
    "            ax.yaxis.set_major_formatter(mpl.dates.DateFormatter('%b'))\n",
    "\n",
    "        axs.append(ax)\n",
    "        ims.append(im)\n",
    "    \n",
    "    # Shared colorbar\n",
    "    ax = fig.add_subplot(gs[i, -1])\n",
    "    cbar = plt.colorbar(im['im'], orientation='vertical', shrink=0.3,\n",
    "                        cax=ax, extend='both')\n",
    "    cbar_labels = ['zonal wind anom. (HF)', 'zonal wind anom. (LF)']\n",
    "    cbar.set_label(cbar_labels[i], size=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paperplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plotspec = [\n",
    "    {'u10_anom_HF': dict(cmap='RdYlBu_r',vmin=-5, vmax=5, step=.5, centercolor=\"#FFFFFF\"),\n",
    "    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=-5, vmax=5, step=.5, centercolor=\"#FFFFFF\")},\n",
    "    {'u10_anom_HF': dict(cmap='RdYlBu_r',vmin=-1.5, vmax=1.5, step=.1, centercolor=\"#FFFFFF\"),\n",
    "    'u10_anom_LF': dict(cmap='RdYlBu_r', vmin=-1.5, vmax=1.5, step=.1, centercolor=\"#FFFFFF\")}\n",
    "]\n",
    "\n",
    "n_columns = 2\n",
    "n_rows = len(weighted_lat_mean.data_vars) #len(weighted_lat_mean['classes'])\n",
    "fig = plt.figure(figsize=(12, n_rows*3))\n",
    "gs = fig.add_gridspec(n_rows, n_columns,\n",
    "                      hspace=0.1, wspace=0.1)\n",
    "axs = []\n",
    "ims = []\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_columns):\n",
    "        idx = gmm_order[j] + 1\n",
    "        var = list(weighted_lat_mean.data_vars)[i]\n",
    "        lat_mean = weighted_lat_mean[var].sel(classes=idx)\n",
    "        lat_mean_masked = xr.where(mask[var].sel(classes=idx) == True, lat_mean, np.nan)\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "\n",
    "        if j == 0:\n",
    "            plkwargs = plotspec[0]\n",
    "        else:\n",
    "            plkwargs = plotspec[1]\n",
    "        \n",
    "        im = gpl.plot_hovmoeller(lat_mean_masked, x='lon', y='time',\n",
    "                                 ax=ax, **plkwargs[var], add_bar=True,\n",
    "                                 cbkwargs=dict(orientation='vertical', shrink=1.0))\n",
    "        # X-axis\n",
    "        ax.set_xlim(None, 100)\n",
    "        if i < n_rows -1:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            xaxis_formatter = FuncFormatter(lambda x, pos: f'{np.where(x < 0, (x % 180),(x % 180 - 180)):.0f}')\n",
    "            ax.xaxis.set_major_formatter(xaxis_formatter)\n",
    "            ax.set_xlabel('longitude') \n",
    "        \n",
    "        # Y-axis to month\n",
    "        if j > 0:\n",
    "            ax.set_yticklabels([])\n",
    "        else:\n",
    "            ax.yaxis.set_major_formatter(mpl.dates.DateFormatter('%b'))\n",
    "        \n",
    "        axs.append(ax)\n",
    "        ims.append(im)\n",
    "\n",
    "        # Colorbar labels\n",
    "        if var == 'u10_anom_HF':\n",
    "            formatter = FuncFormatter(lambda x, pos: f'{x:.1f}')\n",
    "            im['cb'].set_label(f\"zonal wind ano. (HF)\", size=12)\n",
    "        elif var == 'u10_anom_LF':\n",
    "            formatter = FuncFormatter(lambda x, pos: f'{x:.1f}')\n",
    "            im['cb'].set_label(f\"zonal wind ano. (LF)\", size=12)\n",
    "\n",
    "\n",
    "\n",
    "# Add figure numbering\n",
    "# _ = gpl.enumerate_subplots(np.array(axs).T, pos_x=0.02, pos_y=.9, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
